2025-04-11 00:40:45,605 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-04-11 00:40:47,676 - debug_agent_cli - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 00:40:47,676 - src.main - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 00:40:47,676 - src.main - INFO - Using LLM provider: ollama
2025-04-11 00:40:47,706 - src.realtime.context_builder - INFO - Initializing ContextBuilder with log source loki
2025-04-11 00:40:47,706 - src.realtime.debug_plan_creator - INFO - Initializing DebugPlanCreator
2025-04-11 00:40:47,706 - src.realtime.executor - INFO - Initializing Executor (AWS enabled: False)
2025-04-11 00:40:47,706 - src.realtime.analyzer - INFO - Initializing Analyzer
2025-04-11 00:40:47,706 - src.realtime.document_generator - INFO - Initializing DocumentGenerator with format: html
2025-04-11 00:40:47,717 - LiteLLM - INFO - 
LiteLLM completion() model= deepseek-r1:8b; provider = ollama
2025-04-11 00:41:41,137 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-04-11 00:41:41,141 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-04-11 00:41:41,177 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:41:41,178 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:41:41,189 - LiteLLM - INFO - 
LiteLLM completion() model= deepseek-r1:8b; provider = ollama
2025-04-11 00:41:41,205 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:41:41,226 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:42:22,740 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-04-11 00:42:22,742 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-04-11 00:42:22,752 - LiteLLM - INFO - 
LiteLLM completion() model= deepseek-r1:8b; provider = ollama
2025-04-11 00:42:22,771 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:42:22,795 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:43:13,036 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-04-11 00:43:13,037 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-04-11 00:43:13,047 - LiteLLM - INFO - 
LiteLLM completion() model= deepseek-r1:8b; provider = ollama
2025-04-11 00:43:13,073 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:43:13,095 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:43:44,972 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-04-11 00:43:44,973 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-04-11 00:43:44,983 - LiteLLM - INFO - 
LiteLLM completion() model= deepseek-r1:8b; provider = ollama
2025-04-11 00:43:45,002 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:43:45,026 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:44:28,897 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-04-11 00:44:28,899 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-04-11 00:44:28,907 - src.main - INFO - Debugging process completed
2025-04-11 00:44:28,907 - src.main - INFO - Results available at: file:///Users/gajendratiwari/Code/debugging-agents/data/reports/debug_report_TEST-FIXED-123_1744312468.html
2025-04-11 00:44:28,927 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:44:28,950 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:51:08,227 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-04-11 00:51:10,256 - debug_agent_cli - INFO - Starting forecasting pipeline
2025-04-11 00:51:10,256 - src.main - INFO - Setting up forecasting pipeline...
2025-04-11 00:51:10,256 - src.forecasting.service_log_forecaster - INFO - Initializing ServiceLogForecaster with data path: data/service_logs.csv
2025-04-11 00:51:10,256 - src.forecasting.alert_forecaster - INFO - Initializing AlertForecaster with data path: data/alerts.csv
2025-04-11 00:51:10,257 - src.forecasting.service_log_forecaster - WARNING - Log data file not found: data/service_logs.csv
2025-04-11 00:51:10,257 - src.forecasting.service_log_forecaster - INFO - Creating sample service log data
2025-04-11 00:51:10,261 - src.forecasting.service_log_forecaster - INFO - Training Prophet model for service log forecasting
2025-04-11 00:51:10,350 - prophet - INFO - Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.
2025-04-11 00:51:10,356 - cmdstanpy - DEBUG - input tempfile: /var/folders/21/jjln8n357zd9sbcf_vpf561h0000gn/T/tmpvwi4vld7/4vdikzsn.json
2025-04-11 00:51:10,357 - cmdstanpy - DEBUG - input tempfile: /var/folders/21/jjln8n357zd9sbcf_vpf561h0000gn/T/tmpvwi4vld7/13hsfd1t.json
2025-04-11 00:51:10,358 - cmdstanpy - DEBUG - idx 0
2025-04-11 00:51:10,358 - cmdstanpy - DEBUG - running CmdStan, num_threads: None
2025-04-11 00:51:10,358 - cmdstanpy - DEBUG - CmdStan args: ['/Users/gajendratiwari/Code/.venv/lib/python3.12/site-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=36013', 'data', 'file=/var/folders/21/jjln8n357zd9sbcf_vpf561h0000gn/T/tmpvwi4vld7/4vdikzsn.json', 'init=/var/folders/21/jjln8n357zd9sbcf_vpf561h0000gn/T/tmpvwi4vld7/13hsfd1t.json', 'output', 'file=/var/folders/21/jjln8n357zd9sbcf_vpf561h0000gn/T/tmpvwi4vld7/prophet_modelvwbp1jr6/prophet_model-20250411005110.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']
2025-04-11 00:51:10,358 - cmdstanpy - INFO - Chain [1] start processing
2025-04-11 00:51:11,294 - cmdstanpy - INFO - Chain [1] done processing
2025-04-11 00:51:11,295 - src.forecasting.service_log_forecaster - INFO - Prophet model training complete
2025-04-11 00:51:11,891 - src.forecasting.service_log_forecaster - INFO - Forecast plot saved to data/plots/service_forecast_20250411.png
2025-04-11 00:51:11,891 - src.forecasting.service_log_forecaster - INFO - Predicted 0 potential service anomalies
2025-04-11 00:51:11,891 - src.forecasting.alert_forecaster - WARNING - Alert data file not found: data/alerts.csv
2025-04-11 00:51:11,891 - src.forecasting.alert_forecaster - INFO - Creating sample alert data
2025-04-11 00:51:11,895 - src.forecasting.alert_forecaster - INFO - Training alert prediction model
2025-04-11 00:51:11,935 - src.forecasting.alert_forecaster - INFO - Alert prediction model training complete
2025-04-11 00:51:11,936 - src.forecasting.alert_forecaster - INFO - Model saved to data/models/alert_model.pkl
2025-04-11 00:51:11,940 - src.forecasting.alert_forecaster - INFO - Model accuracy: 1.00
2025-04-11 00:51:11,942 - src.forecasting.alert_forecaster - WARNING - Created synthetic target 'incident'
2025-04-11 00:51:11,979 - src.forecasting.alert_forecaster - INFO - Alert prediction plot saved to data/plots/alert_forecast_20250411.png
2025-04-11 00:51:11,979 - src.forecasting.alert_forecaster - INFO - Predicted 0 potential alerts
2025-04-11 00:51:11,979 - src.main - INFO - Forecasting complete. Found 0 potential service anomalies
2025-04-11 00:51:11,979 - src.main - INFO - Alert prediction complete. Found 0 potential alert patterns
2025-04-11 00:51:11,979 - debug_agent_cli - INFO - Forecasting complete. Found 0 potential service anomalies
2025-04-11 00:51:11,979 - debug_agent_cli - INFO - Alert prediction complete. Found 0 potential alert patterns
2025-04-11 00:54:34,213 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-04-11 00:54:36,106 - debug_agent_cli - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 00:54:36,106 - src.main - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 00:54:36,106 - src.main - INFO - Using LLM provider: ollama
2025-04-11 00:54:36,137 - src.realtime.context_builder - INFO - Initializing ContextBuilder with log source loki
2025-04-11 00:54:36,137 - src.realtime.debug_plan_creator - INFO - Initializing DebugPlanCreator
2025-04-11 00:54:36,137 - src.realtime.executor - INFO - Initializing Executor (AWS enabled: False)
2025-04-11 00:54:36,137 - src.realtime.analyzer - INFO - Initializing Analyzer
2025-04-11 00:54:36,137 - src.realtime.document_generator - INFO - Initializing DocumentGenerator with format: html
2025-04-11 00:54:36,149 - LiteLLM - INFO - 
LiteLLM completion() model= deepseek-r1:8b; provider = ollama
2025-04-11 00:55:23,310 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-04-11 00:55:23,315 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-04-11 00:55:23,344 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:55:23,345 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:55:23,362 - LiteLLM - INFO - 
LiteLLM completion() model= deepseek-r1:8b; provider = ollama
2025-04-11 00:55:23,377 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:55:23,401 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:56:08,219 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-04-11 00:56:08,221 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-04-11 00:56:08,231 - LiteLLM - INFO - 
LiteLLM completion() model= deepseek-r1:8b; provider = ollama
2025-04-11 00:56:08,249 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:56:08,269 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:56:55,305 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-04-11 00:56:55,309 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-04-11 00:56:55,322 - LiteLLM - INFO - 
LiteLLM completion() model= deepseek-r1:8b; provider = ollama
2025-04-11 00:56:55,340 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:56:55,362 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:57:39,562 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-04-11 00:57:39,564 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-04-11 00:57:39,582 - LiteLLM - INFO - 
LiteLLM completion() model= deepseek-r1:8b; provider = ollama
2025-04-11 00:57:39,599 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:57:39,629 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:58:19,266 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-04-11 00:58:19,270 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-04-11 00:58:19,282 - src.main - INFO - Debugging process completed
2025-04-11 00:58:19,282 - src.main - INFO - Results available at: file:///Users/gajendratiwari/Code/debugging-agents/data/reports/debug_report_TEST-FIXED-123_1744313299.html
2025-04-11 00:58:19,303 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 00:58:19,327 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-04-11 19:02:11 - debug_agent_cli - INFO - Logging initialized with configuration from: /Users/gajendratiwari/Code/debugging-agents/config/logging.yaml
2025-04-11 19:02:14 - debug_agent_cli - INFO - Starting real-time debugging for issue: YOUR-ISSUE-123
2025-04-11 19:02:14 - src.main - INFO - Starting real-time debugging for issue: YOUR-ISSUE-123
2025-04-11 19:02:14 - src.main - INFO - Using LLM provider: bedrock
2025-04-11 19:02:14 - botocore.credentials - INFO - Found credentials in environment variables.
2025-04-11 19:02:14 - src.main - ERROR - Error during debugging process: 1 validation error for BedrockChat
temperature
  Extra inputs are not permitted [type=extra_forbidden, input_value=0.2, input_type=float]
    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden
2025-04-11 19:03:38 - debug_agent_cli - INFO - Logging initialized with configuration from: /Users/gajendratiwari/Code/debugging-agents/config/logging.yaml
2025-04-11 19:03:44 - debug_agent_cli - INFO - Logging initialized with configuration from: /Users/gajendratiwari/Code/debugging-agents/config/logging.yaml
2025-04-11 19:03:47 - debug_agent_cli - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 19:03:47 - src.main - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 19:03:47 - src.main - INFO - Using LLM provider: bedrock
2025-04-11 19:03:47 - src.realtime.context_builder - INFO - Initializing ContextBuilder with log source loki
2025-04-11 19:03:47 - src.realtime.debug_plan_creator - INFO - Initializing DebugPlanCreator
2025-04-11 19:03:47 - src.realtime.executor - INFO - Initializing Executor (AWS enabled: False)
2025-04-11 19:03:47 - src.realtime.analyzer - INFO - Initializing Analyzer
2025-04-11 19:03:47 - src.realtime.document_generator - INFO - Initializing DocumentGenerator with format: html
2025-04-11 19:03:47 - root - ERROR - LiteLLM call failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=client=<botocore.client.BedrockRuntime object at 0x14ac1bfb0> region_name='us-west-2' aws_access_key_id=SecretStr('**********') aws_secret_access_key=SecretStr('**********') model_id='deepseek.r1-v1:0' model_kwargs={} temperature=0.2
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-04-11 19:03:47 - src.main - ERROR - Error during debugging process: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=client=<botocore.client.BedrockRuntime object at 0x14ac1bfb0> region_name='us-west-2' aws_access_key_id=SecretStr('**********') aws_secret_access_key=SecretStr('**********') model_id='deepseek.r1-v1:0' model_kwargs={} temperature=0.2
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-04-11 19:04:33 - debug_agent_cli - INFO - Logging initialized with configuration from: /Users/gajendratiwari/Code/debugging-agents/config/logging.yaml
2025-04-11 19:04:36 - debug_agent_cli - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 19:04:36 - src.main - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 19:04:36 - src.main - INFO - Using LLM provider: bedrock
2025-04-11 19:04:36 - src.realtime.context_builder - INFO - Initializing ContextBuilder with log source loki
2025-04-11 19:04:36 - src.realtime.debug_plan_creator - INFO - Initializing DebugPlanCreator
2025-04-11 19:04:36 - src.realtime.executor - INFO - Initializing Executor (AWS enabled: False)
2025-04-11 19:04:36 - src.realtime.analyzer - INFO - Initializing Analyzer
2025-04-11 19:04:36 - src.realtime.document_generator - INFO - Initializing DocumentGenerator with format: html
2025-04-11 19:04:36 - root - ERROR - LiteLLM call failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=client=<botocore.client.BedrockRuntime object at 0x12ff3b8c0> region_name='us-west-2' aws_access_key_id=SecretStr('**********') aws_secret_access_key=SecretStr('**********') model_id='deepseek.r1-v1:0' model_kwargs={} temperature=0.2
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-04-11 19:04:36 - src.main - ERROR - Error during debugging process: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=client=<botocore.client.BedrockRuntime object at 0x12ff3b8c0> region_name='us-west-2' aws_access_key_id=SecretStr('**********') aws_secret_access_key=SecretStr('**********') model_id='deepseek.r1-v1:0' model_kwargs={} temperature=0.2
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-04-11 19:06:24 - debug_agent_cli - INFO - Logging initialized with configuration from: /Users/gajendratiwari/Code/debugging-agents/config/logging.yaml
2025-04-11 19:06:27 - debug_agent_cli - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 19:06:27 - src.main - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 19:06:27 - src.main - INFO - Using LLM provider: bedrock
2025-04-11 19:06:27 - src.main - ERROR - Error during debugging process: cannot import name 'ToolsCache' from 'crewai.agents.cache' (/Users/gajendratiwari/Code/.venv/lib/python3.12/site-packages/crewai/agents/cache/__init__.py)
2025-04-11 19:07:07 - debug_agent_cli - INFO - Logging initialized with configuration from: /Users/gajendratiwari/Code/debugging-agents/config/logging.yaml
2025-04-11 19:07:13 - debug_agent_cli - INFO - Logging initialized with configuration from: /Users/gajendratiwari/Code/debugging-agents/config/logging.yaml
2025-04-11 19:07:15 - debug_agent_cli - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 19:07:15 - src.main - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 19:07:15 - src.main - INFO - Using LLM provider: bedrock
2025-04-11 19:07:15 - src.coordination.crew_manager - INFO - Initializing DebugCrew with LLM provider: bedrock
2025-04-11 19:07:15 - src.coordination.crew_manager - INFO - Set CREW_LLM_PROVIDER=bedrock for CrewAI
2025-04-11 19:07:15 - src.realtime.context_builder - INFO - Initializing ContextBuilder with log source loki
2025-04-11 19:07:15 - src.realtime.debug_plan_creator - INFO - Initializing DebugPlanCreator
2025-04-11 19:07:15 - src.realtime.executor - INFO - Initializing Executor (AWS enabled: False)
2025-04-11 19:07:15 - src.realtime.analyzer - INFO - Initializing Analyzer
2025-04-11 19:07:15 - src.realtime.document_generator - INFO - Initializing DocumentGenerator with format: html
2025-04-11 19:07:15 - src.coordination.crew_manager - INFO - Running debugging process for issue TEST-FIXED-123
2025-04-11 19:07:15 - src.main - ERROR - Error during debugging process: 'Agent' object has no attribute 'name'
2025-04-11 19:08:12 - debug_agent_cli - INFO - Logging initialized with configuration from: /Users/gajendratiwari/Code/debugging-agents/config/logging.yaml
2025-04-11 19:08:14 - debug_agent_cli - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 19:08:14 - src.main - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 19:08:14 - src.main - INFO - Using LLM provider: bedrock
2025-04-11 19:08:14 - src.coordination.crew_manager - INFO - Initializing DebugCrew with LLM provider: bedrock
2025-04-11 19:08:14 - src.coordination.crew_manager - INFO - Set CREW_LLM_PROVIDER=bedrock for CrewAI
2025-04-11 19:08:14 - src.realtime.context_builder - INFO - Initializing ContextBuilder with log source loki
2025-04-11 19:08:14 - src.realtime.debug_plan_creator - INFO - Initializing DebugPlanCreator
2025-04-11 19:08:14 - src.realtime.executor - INFO - Initializing Executor (AWS enabled: False)
2025-04-11 19:08:14 - src.realtime.analyzer - INFO - Initializing Analyzer
2025-04-11 19:08:14 - src.realtime.document_generator - INFO - Initializing DocumentGenerator with format: html
2025-04-11 19:08:14 - src.coordination.crew_manager - INFO - Running debugging process for issue TEST-FIXED-123
2025-04-11 19:08:14 - src.main - ERROR - Error during debugging process: 'Agent' object has no attribute 'name'
2025-04-11 19:08:57 - debug_agent_cli - INFO - Logging initialized with configuration from: /Users/gajendratiwari/Code/debugging-agents/config/logging.yaml
2025-04-11 19:09:00 - debug_agent_cli - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 19:09:00 - src.main - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 19:09:00 - src.main - INFO - Using LLM provider: bedrock
2025-04-11 19:09:00 - src.coordination.crew_manager - INFO - Initializing DebugCrew with LLM provider: bedrock
2025-04-11 19:09:00 - src.coordination.crew_manager - INFO - Set CREW_LLM_PROVIDER=bedrock for CrewAI
2025-04-11 19:09:00 - src.realtime.context_builder - INFO - Initializing ContextBuilder with log source loki
2025-04-11 19:09:00 - src.realtime.debug_plan_creator - INFO - Initializing DebugPlanCreator
2025-04-11 19:09:00 - src.realtime.executor - INFO - Initializing Executor (AWS enabled: False)
2025-04-11 19:09:00 - src.realtime.analyzer - INFO - Initializing Analyzer
2025-04-11 19:09:00 - src.realtime.document_generator - INFO - Initializing DocumentGenerator with format: html
2025-04-11 19:09:00 - src.coordination.crew_manager - INFO - Running debugging process for issue TEST-FIXED-123
2025-04-11 19:09:00 - src.coordination.crew_manager - INFO - Starting crew kickoff
2025-04-11 19:09:00 - root - ERROR - LiteLLM call failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=client=<botocore.client.BedrockRuntime object at 0x141c726c0> region_name='us-west-2' aws_access_key_id=SecretStr('**********') aws_secret_access_key=SecretStr('**********') model_id='deepseek.r1-v1:0' model_kwargs={} temperature=0.2
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-04-11 19:09:00 - src.coordination.crew_manager - ERROR - Error during crew execution: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=client=<botocore.client.BedrockRuntime object at 0x141c726c0> region_name='us-west-2' aws_access_key_id=SecretStr('**********') aws_secret_access_key=SecretStr('**********') model_id='deepseek.r1-v1:0' model_kwargs={} temperature=0.2
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-04-11 19:09:00 - src.coordination.crew_manager - INFO - Generating report at /Users/gajendratiwari/Code/debugging-agents/data/reports/debug_report_TEST-FIXED-123_1744378740.html
2025-04-11 19:09:00 - src.coordination.crew_manager - INFO - Report generated at /Users/gajendratiwari/Code/debugging-agents/data/reports/debug_report_TEST-FIXED-123_1744378740.html
2025-04-11 19:09:00 - src.main - INFO - Debugging process completed
2025-04-11 19:09:00 - src.main - INFO - Results available at: file:///Users/gajendratiwari/Code/debugging-agents/data/reports/debug_report_TEST-FIXED-123_1744378740.html
2025-04-11 19:10:13 - debug_agent_cli - INFO - Logging initialized with configuration from: /Users/gajendratiwari/Code/debugging-agents/config/logging.yaml
2025-04-11 19:10:16 - debug_agent_cli - INFO - Starting real-time debugging for issue: YOUR-ISSUE-123
2025-04-11 19:10:16 - src.main - INFO - Starting real-time debugging for issue: YOUR-ISSUE-123
2025-04-11 19:10:16 - src.main - INFO - Using LLM provider: bedrock
2025-04-11 19:10:16 - src.coordination.crew_manager - INFO - Initializing DebugCrew with LLM provider: bedrock
2025-04-11 19:10:16 - src.coordination.crew_manager - INFO - Set CREW_LLM_PROVIDER=bedrock for CrewAI
2025-04-11 19:10:16 - src.realtime.context_builder - INFO - Initializing ContextBuilder with log source loki
2025-04-11 19:10:16 - src.realtime.debug_plan_creator - INFO - Initializing DebugPlanCreator
2025-04-11 19:10:16 - src.realtime.executor - INFO - Initializing Executor (AWS enabled: False)
2025-04-11 19:10:16 - src.realtime.analyzer - INFO - Initializing Analyzer
2025-04-11 19:10:16 - src.realtime.document_generator - INFO - Initializing DocumentGenerator with format: html
2025-04-11 19:10:16 - src.coordination.crew_manager - INFO - Running debugging process for issue YOUR-ISSUE-123
2025-04-11 19:10:16 - src.coordination.crew_manager - INFO - Starting crew kickoff
2025-04-11 19:10:16 - root - ERROR - LiteLLM call failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=client=<botocore.client.BedrockRuntime object at 0x144bd3980> region_name='us-west-2' aws_access_key_id=SecretStr('**********') aws_secret_access_key=SecretStr('**********') model_id='deepseek.r1-v1:0' model_kwargs={} temperature=0.2
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-04-11 19:10:16 - src.coordination.crew_manager - ERROR - Error during crew execution: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=client=<botocore.client.BedrockRuntime object at 0x144bd3980> region_name='us-west-2' aws_access_key_id=SecretStr('**********') aws_secret_access_key=SecretStr('**********') model_id='deepseek.r1-v1:0' model_kwargs={} temperature=0.2
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-04-11 19:10:16 - src.coordination.crew_manager - INFO - Generating report at /Users/gajendratiwari/Code/debugging-agents/data/reports/debug_report_YOUR-ISSUE-123_1744378816.html
2025-04-11 19:10:16 - src.coordination.crew_manager - INFO - Report generated at /Users/gajendratiwari/Code/debugging-agents/data/reports/debug_report_YOUR-ISSUE-123_1744378816.html
2025-04-11 19:10:16 - src.main - INFO - Debugging process completed
2025-04-11 19:10:16 - src.main - INFO - Results available at: file:///Users/gajendratiwari/Code/debugging-agents/data/reports/debug_report_YOUR-ISSUE-123_1744378816.html
2025-04-11 19:13:54 - debug_agent_cli - INFO - Logging initialized with configuration from: /Users/gajendratiwari/Code/debugging-agents/config/logging.yaml
2025-04-11 19:14:03 - debug_agent_cli - INFO - Logging initialized with configuration from: /Users/gajendratiwari/Code/debugging-agents/config/logging.yaml
2025-04-11 19:14:06 - debug_agent_cli - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 19:14:06 - src.main - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 19:14:06 - src.main - INFO - Using LLM provider: bedrock
2025-04-11 19:14:06 - src.coordination.crew_manager - INFO - Initializing DebugCrew with LLM provider: bedrock
2025-04-11 19:14:06 - src.utils.llm_factory - INFO - Creating DirectBedrockClient with model=deepseek.r1-v1:0, region=us-west-2
2025-04-11 19:14:06 - src.utils.llm_factory - INFO - Initialized DirectBedrockClient with model: bedrock/deepseek.r1-v1:0
2025-04-11 19:14:06 - src.coordination.crew_manager - INFO - Set CREW_LLM_PROVIDER=bedrock for CrewAI
2025-04-11 19:14:06 - src.realtime.context_builder - INFO - Initializing ContextBuilder with log source loki
2025-04-11 19:14:06 - src.realtime.debug_plan_creator - INFO - Initializing DebugPlanCreator
2025-04-11 19:14:06 - src.realtime.executor - INFO - Initializing Executor (AWS enabled: False)
2025-04-11 19:14:06 - src.realtime.analyzer - INFO - Initializing Analyzer
2025-04-11 19:14:06 - src.realtime.document_generator - INFO - Initializing DocumentGenerator with format: html
2025-04-11 19:14:06 - src.coordination.crew_manager - INFO - Creating Bedrock-compatible agent for ContextBuilder
2025-04-11 19:14:06 - src.coordination.crew_manager - INFO - Creating Bedrock-compatible agent for DebugPlanCreator
2025-04-11 19:14:06 - src.coordination.crew_manager - INFO - Creating Bedrock-compatible agent for Executor
2025-04-11 19:14:06 - src.coordination.crew_manager - INFO - Creating Bedrock-compatible agent for Analyzer
2025-04-11 19:14:06 - src.coordination.crew_manager - INFO - Creating Bedrock-compatible agent for DocumentGenerator
2025-04-11 19:14:06 - src.coordination.crew_manager - INFO - Running debugging process for issue TEST-FIXED-123
2025-04-11 19:14:06 - src.coordination.crew_manager - INFO - Starting crew kickoff
2025-04-11 19:14:06 - LiteLLM - INFO - 
LiteLLM completion() model= gpt-4o-mini; provider = openai
2025-04-11 19:14:07 - root - ERROR - LiteLLM call failed: litellm.AuthenticationError: AuthenticationError: OpenAIException - Incorrect API key provided: sk-test1****6789. You can find your API key at https://platform.openai.com/account/api-keys.
2025-04-11 19:14:07 - src.coordination.crew_manager - ERROR - Error during crew execution: litellm.AuthenticationError: AuthenticationError: OpenAIException - Incorrect API key provided: sk-test1****6789. You can find your API key at https://platform.openai.com/account/api-keys.
2025-04-11 19:14:07 - src.coordination.crew_manager - INFO - Generating report at /Users/gajendratiwari/Code/debugging-agents/data/reports/debug_report_TEST-FIXED-123_1744379047.html
2025-04-11 19:14:07 - src.coordination.crew_manager - INFO - Report generated at /Users/gajendratiwari/Code/debugging-agents/data/reports/debug_report_TEST-FIXED-123_1744379047.html
2025-04-11 19:14:07 - src.main - INFO - Debugging process completed
2025-04-11 19:14:07 - src.main - INFO - Results available at: file:///Users/gajendratiwari/Code/debugging-agents/data/reports/debug_report_TEST-FIXED-123_1744379047.html
2025-04-11 19:15:08 - debug_agent_cli - INFO - Logging initialized with configuration from: /Users/gajendratiwari/Code/debugging-agents/config/logging.yaml
2025-04-11 19:15:10 - debug_agent_cli - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 19:15:10 - src.main - INFO - Starting real-time debugging for issue: TEST-FIXED-123
2025-04-11 19:15:10 - src.main - INFO - Using LLM provider: bedrock
2025-04-11 19:15:10 - src.coordination.crew_manager - INFO - Initializing DebugCrew with LLM provider: bedrock
2025-04-11 19:15:10 - src.utils.llm_factory - INFO - Creating DirectBedrockClient with model=deepseek.r1-v1:0, region=us-west-2
2025-04-11 19:15:10 - src.utils.llm_factory - INFO - Initialized DirectBedrockClient with model: bedrock/deepseek.r1-v1:0
2025-04-11 19:15:10 - src.coordination.crew_manager - INFO - Set CREW_LLM_PROVIDER=bedrock for CrewAI
2025-04-11 19:15:10 - src.realtime.context_builder - INFO - Initializing ContextBuilder with log source loki
2025-04-11 19:15:10 - src.realtime.debug_plan_creator - INFO - Initializing DebugPlanCreator
2025-04-11 19:15:10 - src.realtime.executor - INFO - Initializing Executor (AWS enabled: False)
2025-04-11 19:15:10 - src.realtime.analyzer - INFO - Initializing Analyzer
2025-04-11 19:15:10 - src.realtime.document_generator - INFO - Initializing DocumentGenerator with format: html
2025-04-11 19:15:10 - src.coordination.crew_manager - INFO - Creating Bedrock-compatible agent for ContextBuilder
2025-04-11 19:15:10 - src.coordination.crew_manager - INFO - Creating Bedrock-compatible agent for DebugPlanCreator
2025-04-11 19:15:10 - src.coordination.crew_manager - INFO - Creating Bedrock-compatible agent for Executor
2025-04-11 19:15:10 - src.coordination.crew_manager - INFO - Creating Bedrock-compatible agent for Analyzer
2025-04-11 19:15:10 - src.coordination.crew_manager - INFO - Creating Bedrock-compatible agent for DocumentGenerator
2025-04-11 19:15:10 - src.coordination.crew_manager - INFO - Running debugging process for issue TEST-FIXED-123
2025-04-11 19:15:10 - src.coordination.crew_manager - INFO - Starting crew kickoff
2025-04-11 19:15:10 - LiteLLM - INFO - 
LiteLLM completion() model= gpt-4o-mini; provider = openai
2025-04-11 19:15:11 - root - ERROR - LiteLLM call failed: litellm.AuthenticationError: AuthenticationError: OpenAIException - Incorrect API key provided: sk-valid************************rock. You can find your API key at https://platform.openai.com/account/api-keys.
2025-04-11 19:15:11 - src.coordination.crew_manager - ERROR - Error during crew execution: litellm.AuthenticationError: AuthenticationError: OpenAIException - Incorrect API key provided: sk-valid************************rock. You can find your API key at https://platform.openai.com/account/api-keys.
2025-04-11 19:15:11 - src.coordination.crew_manager - INFO - Generating report at /Users/gajendratiwari/Code/debugging-agents/data/reports/debug_report_TEST-FIXED-123_1744379111.html
2025-04-11 19:15:11 - src.coordination.crew_manager - INFO - Report generated at /Users/gajendratiwari/Code/debugging-agents/data/reports/debug_report_TEST-FIXED-123_1744379111.html
2025-04-11 19:15:11 - src.main - INFO - Debugging process completed
2025-04-11 19:15:11 - src.main - INFO - Results available at: file:///Users/gajendratiwari/Code/debugging-agents/data/reports/debug_report_TEST-FIXED-123_1744379111.html
2025-04-11 19:20:12 - debug_agent_cli - INFO - Logging initialized with configuration from: /Users/gajendratiwari/Code/debugging-agents/config/logging.yaml
2025-04-11 19:20:15 - debug_agent_cli - INFO - Starting real-time debugging for issue: YOUR-ISSUE-123
2025-04-11 19:20:15 - src.main - INFO - Starting real-time debugging for issue: YOUR-ISSUE-123
2025-04-11 19:20:15 - src.main - INFO - Using LLM provider: bedrock
2025-04-11 19:20:15 - src.main - ERROR - Error during debugging process: DebugCrew.__init__() got an unexpected keyword argument 'llm_provider'
2025-04-11 19:20:25 - debug_agent_cli - INFO - Logging initialized with configuration from: /Users/gajendratiwari/Code/debugging-agents/config/logging.yaml
2025-04-11 19:20:27 - debug_agent_cli - INFO - Starting real-time debugging for issue: YOUR-ISSUE-123
2025-04-11 19:20:27 - src.main - INFO - Starting real-time debugging for issue: YOUR-ISSUE-123
2025-04-11 19:20:27 - src.main - INFO - Using LLM provider: bedrock
2025-04-11 19:20:27 - src.main - ERROR - Error during debugging process: DebugCrew.__init__() got an unexpected keyword argument 'llm_provider'
